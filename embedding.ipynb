{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca6a1be",
   "metadata": {},
   "source": [
    "# Embedding Pipelines (1. VLM captioning; 2. CLIP visual embedding)\n",
    "\n",
    "## 1. VLM captioning (example: Waymo dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1028e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the name of all images to be captioned\n",
    "# only captioning fron-viewing images\n",
    "\n",
    "import os\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    try:\n",
    "        # list all files and directories in the specified directory\n",
    "        files_and_dirs = os.listdir(folder_path)\n",
    "        \n",
    "        # filter out directories, keep only files\n",
    "        files = [f for f in files_and_dirs if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        \n",
    "        return files\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "folder_path = './image_0/'\n",
    "files = list_files_in_folder(folder_path)\n",
    "files.sort()\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e623ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trfiles = files[:31617]    # training images\n",
    "vlfiles = files[31617:]    # validation images\n",
    "\n",
    "# only captioning training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "import pickle as pkl\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pretrained = \"lmms-lab/llama3-llava-next-8b\"\n",
    "model_name = \"llava_llama3\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\"\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map) # Add any other thing you want to pass in llava_model_args\n",
    "\n",
    "model.eval()\n",
    "model.tie_weights()\n",
    "\n",
    "trresults = []\n",
    "import pickle\n",
    "counter = 1\n",
    "\n",
    "for i in tqdm(trfiles):\n",
    "    image = Image.open('./image_0/'+i).convert('RGB')\n",
    "\n",
    "\n",
    "    image_tensor = process_images([image], image_processor, model.config)\n",
    "    image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "    \n",
    "    conv_template = \"llava_llama_3\" # Make sure you use correct chat template for different models\n",
    "    \n",
    "    with open('./questions/av_driving_condition_description.yaml') as f:\n",
    "        questions = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    # Prompt: \n",
    "    # \n",
    "    # Generic AV Prompt from [Shen, M., Chang, N., Liu, S., & Alvarez, J. M. (2024). SSE: Multimodal Semantic \n",
    "    # Data Selection and Enrichment for Industrial-scale Data Assimilation. arXiv preprint arXiv:2409.13860.]\n",
    "    #\n",
    "    # The image is taken from inside the ego vehicle looking out through the windshield onto a road and you are\n",
    "    # the driver of the ego vehicle. Describe the driving condition shown in the image in 150 words\n",
    "    \n",
    "    question = DEFAULT_IMAGE_TOKEN + \"\\n\" + questions[\"common\"][::-1].pop()\n",
    "    conv = copy.deepcopy(conv_templates[conv_template])\n",
    "    conv.append_message(conv.roles[0], question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt_question = conv.get_prompt()\n",
    "    \n",
    "    input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    cont=model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        image_sizes=[image.size],\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    \n",
    "    text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "    answer = text_outputs[0].replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "    answer = answer.replace(\"<|startoftext|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "    print(answer)\n",
    "\n",
    "    trresults.append(answer)\n",
    "    if counter%1000==0:\n",
    "        with open(\"./trllava16b8l3\"+str(counter)+\".pkl\", 'wb') as file:  # 'wb' mode opens the file in binary write mode\n",
    "            pickle.dump(trresults, file, protocol=4)\n",
    "    counter += 1\n",
    "    \n",
    "with open(\"./trllava16b8l3-final.pkl\", 'wb') as file:  # 'wb' mode opens the file in binary write mode\n",
    "    pickle.dump(trresults, file, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dea2f3",
   "metadata": {},
   "source": [
    "### Embed the captions with SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c792a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "sentences = trresults\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "with open(\"./capemb.pkl\", 'wb') as file:  # 'wb' mode opens the file in binary write mode\n",
    "    pickle.dump(embeddings, file, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d0fe0",
   "metadata": {},
   "source": [
    "## 2. CLIP visual embedding (example: Waymo dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2fc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only embedding training images\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained CLIP model and processor\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Load and process images\n",
    "embeddings = []\n",
    "for filename in tqdm(trfiles):\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "\n",
    "    # Preprocess image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get image embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embedding = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    # Store in dictionary\n",
    "    embeddings.append(embedding.cpu().numpy())\n",
    "\n",
    "    \n",
    "with open(\"./trclip.pkl\", 'wb') as file:  # 'wb' mode opens the file in binary write mode\n",
    "    pickle.dump(embeddings, file, protocol=4)\n",
    "\n",
    "embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39h",
   "language": "python",
   "name": "p39h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
